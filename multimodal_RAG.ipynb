{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973de2d3",
   "metadata": {},
   "source": [
    "# Multimodal Rag (PDF with Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f020d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "from langchain_core.documents import Document\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os \n",
    "import base64\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbf744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CLIP MODEL-Contrast Language Image PreTraining\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## Setup the environment\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#initialiazed the clip model for unified embeddings\n",
    "clip_model=CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666997d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding functions\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data,str): #if path\n",
    "        image=Image.open(image_data).convert(\"RGB\")\n",
    "    else: #IF PIL IMAGE\n",
    "        image=image_data\n",
    "    \n",
    "    clip_processor(images=image,return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_image_features(**inputs)\n",
    "\n",
    "        #normalize Embeddings\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    inputs= clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77 #Clips max token length\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features= clip_model.get_text_features(**inputs)\n",
    "\n",
    "        #Normalize embeddings\n",
    "        features =features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b692a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"M:\\job hunt\\Harrisburg University Documents\\Sem2\\Scientific Compuitng 1\\Final Research Project\\Project_data_analysis.pdf\"\n",
    "\n",
    "doc=fitz.open(pdf_path)\n",
    "\n",
    "## Storage for all documents and embeddings\n",
    "\n",
    "all_docs=[]\n",
    "all_embeddings=[]\n",
    "image_data_store={} #store actual data for LLM\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c50b509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('M:\\job hunt\\Harrisburg University Documents\\Sem2\\Scientific Compuitng 1\\Final Research Project\\Project_data_analysis.pdf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b9daf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 0 on page 0: name 'inputs' is not defined\n",
      "Error processing image 1 on page 0: name 'inputs' is not defined\n",
      "Error processing image 2 on page 0: name 'inputs' is not defined\n",
      "Error processing image 3 on page 0: name 'inputs' is not defined\n",
      "Error processing image 0 on page 1: name 'inputs' is not defined\n",
      "Error processing image 1 on page 1: name 'inputs' is not defined\n",
      "Error processing image 2 on page 1: name 'inputs' is not defined\n",
      "Error processing image 3 on page 1: name 'inputs' is not defined\n",
      "Error processing image 0 on page 2: name 'inputs' is not defined\n",
      "Error processing image 1 on page 2: name 'inputs' is not defined\n",
      "Error processing image 2 on page 2: name 'inputs' is not defined\n",
      "Error processing image 3 on page 2: name 'inputs' is not defined\n",
      "Error processing image 0 on page 3: name 'inputs' is not defined\n",
      "Error processing image 1 on page 3: name 'inputs' is not defined\n",
      "Error processing image 2 on page 3: name 'inputs' is not defined\n",
      "Error processing image 3 on page 3: name 'inputs' is not defined\n",
      "Error processing image 0 on page 4: name 'inputs' is not defined\n",
      "Error processing image 1 on page 4: name 'inputs' is not defined\n",
      "Error processing image 2 on page 4: name 'inputs' is not defined\n",
      "Error processing image 3 on page 4: name 'inputs' is not defined\n",
      "Error processing image 0 on page 5: name 'inputs' is not defined\n",
      "Error processing image 1 on page 5: name 'inputs' is not defined\n",
      "Error processing image 2 on page 5: name 'inputs' is not defined\n",
      "Error processing image 3 on page 5: name 'inputs' is not defined\n"
     ]
    }
   ],
   "source": [
    "for i, page in enumerate(doc):\n",
    "    ## process text\n",
    "    text = page.get_text()\n",
    "\n",
    "    if text.strip():\n",
    "        #create a temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={'page': i, \"type\": 'text'})\n",
    "\n",
    "        # keep your original call name if that's what your splitter uses\n",
    "        try:\n",
    "            text_chunks = splitter.split_documents([temp_doc])  # LangChain standard\n",
    "        except AttributeError:\n",
    "            text_chunks = splitter.split_document([temp_doc])   # your original method name\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            # prefer your original target; fall back if it's not defined\n",
    "            try:\n",
    "                all.docs.append(chunk)      # if you truly have `all.docs`\n",
    "            except Exception:\n",
    "                all_docs.append(chunk)      # common alternative\n",
    "\n",
    "    ## process the images\n",
    "    # Three Important Actions:\n",
    "\n",
    "    ##convert PDF image to PIL format\n",
    "    ##Store as base64 for gpt-4v (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrival \n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image['image']\n",
    "\n",
    "            #convert to PIL\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "            \n",
    "            # create unique identifier\n",
    "            image_id = f'page_{i}_img_{img_index}'\n",
    "\n",
    "            #store image as base64 for later use with GPT4-V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format='PNG')\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "\n",
    "            #Embed image using CLIP \n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            # create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image:{image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "\n",
    "            # prefer your original target; fall back if it's not defined\n",
    "            try:\n",
    "                all.docs.append(image_doc)\n",
    "            except Exception:\n",
    "                all_docs.append(image_doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing image {img_index} on page {i}: {e}')\n",
    "            continue\n",
    "\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809141af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
